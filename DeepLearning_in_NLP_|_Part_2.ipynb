{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning in NLP | Part 2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLepI7QjBZxubPLTyfOwzt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuvaraj-Premlal/NLP_Deep_Learning/blob/main/DeepLearning_in_NLP_%7C_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Embeddings**\n",
        "-  Another way to associate a vector with a word \n",
        "-  Generally these are dense word vectors\n",
        "-  WE vectors are low dimensional floating point vectors opposed to OHE vectors that are binary, sparse and very high dimensional\n",
        "-  WE vectors are learned from data\n",
        "\n",
        "WE methods\n",
        "\n",
        "-  Type 1 : Learn WE jointly with the main task\n",
        "-  Type 2 : Use pretrained word embeddings"
      ],
      "metadata": {
        "id": "8wdPtANKizcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE with Embedding Layers\n",
        "\n",
        "-  Simplest way to associate a dense vector with a word is to choose the vector at random\n",
        "\n",
        "-  This will lead to unstructured embedding space that may not recognize similar words as well\n",
        "\n",
        "-  In Good WE, the geometric relationships between word vectors should reflect the semantic relationship between these words.\n",
        "\n",
        "- WE are meant to map human language in to geometric space.\n",
        "\n"
      ],
      "metadata": {
        "id": "Pw-QBIGGkBTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(1000,64) # Number of possible tokens = 1000 ; Dimensionality of Embeddings = 64\n",
        "\n",
        "# embedding layer is a dictionary that maps integer indices to dense vectors\n",
        "# it takes integers as input , it looks up these integers in an internal dictionary, and it returns corresponding vectors\n",
        "# Word index -> Embedding layer -> Corresponding word vector"
      ],
      "metadata": {
        "id": "nBdSbuvxlW6U"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  Embedding layer takes a 2D tensor of integers, of shape (samples, sequence_length) where each entry is a sequence of integers.\n",
        "\n",
        "-  It can embed sequences of variable lengths: for instance, you could feed in to the Embedding layer in the previous example batches with shapes (32,10) [ batch of 32 sequences of length 10]\n",
        "\n",
        "-  All sequences in a batch must have the same length; shorter it is , padding with zeros is necessary ; longer it is, truncation is necessary\n",
        "\n",
        "- This embedding layer returns a 3D tensor of shape (samples, sequence_length, embedding_dimensionality)\n",
        "\n",
        "- When an embedding layer is instantiated, its weights are initially random, just with any other layer."
      ],
      "metadata": {
        "id": "qPS2FjjpmTqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMDB movie review sentiment prediction task\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 20\n",
        "\n",
        "(x_train, y_train),(x_test,y_test) = imdb.load_data(num_words = max_features)\n",
        "#print(x_train,y_train)\n",
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuTS4KdAnutK",
        "outputId": "315f6804-5b7a-4c1e-8c54-c5eb88301cd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DECODE before padding\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n",
        "decoded_review = ''.join([reverse_word_index.get(i-3,'?') for i in x_train[1]])\n",
        "decoded_review"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "wr91Fk9pqOsO",
        "outputId": "f11aa582-dc0a-455b-ba35-637209d32c4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"?bighairbigboobsbadmusicandagiantsafetypinthesearethewordstobestdescribethisterriblemovieilovecheesyhorrormoviesandi'veseenhundredsbutthishadgottobeonoftheworstevermadetheplotispaperthinandridiculoustheactingisanabominationthescriptiscompletelylaughablethebestistheendshowdownwiththecopandhowheworkedoutwhothekillerisit'sjustsodamnterriblywrittentheclothesaresickeningandfunnyinequal?thehairisbiglotsofboobs?menwearthosecut?shirtsthatshowofftheir?sickeningthatmenactuallyworethemandthemusicisjust?trashthatplaysoverandoveragaininalmosteveryscenethereistrashymusicboobsand?takingawaybodiesandthegymstilldoesn'tclosefor?alljokingasidethisisatrulybadfilmwhoseonlycharmistolookbackonthedisasterthatwasthe80'sandhaveagoodoldlaughathowbadeverythingwasbackthen\""
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=maxlen)\n",
        "\n",
        "print(x_train,y_train)\n",
        "x_train[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa2IDO-soVxy",
        "outputId": "cf86fc6a-95b2-464b-ef4e-456e14762b7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  65   16   38 ...   19  178   32]\n",
            " [  23    4 1690 ...   16  145   95]\n",
            " [1352   13  191 ...    7  129  113]\n",
            " ...\n",
            " [  11 1818 7561 ...    4 3586    2]\n",
            " [  92  401  728 ...   12    9   23]\n",
            " [ 764   40    4 ...  204  131    9]] [1 0 0 ... 0 1 0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  23,    4, 1690,   15,   16,    4, 1355,    5,   28,    6,   52,\n",
              "        154,  462,   33,   89,   78,  285,   16,  145,   95], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DECODE after padding\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n",
        "decoded_review = ''.join([reverse_word_index.get(i-3,'?') for i in x_train[1]])\n",
        "decoded_review"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "01SHJsUErBWP",
        "outputId": "6e1153fd-d752-47fc-c4fc-10ae8b3a323c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"onthedisasterthatwasthe80'sandhaveagoodoldlaughathowbadeverythingwasbackthen\""
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape,y_train.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6OLWYOwv8cq",
        "outputId": "779811d2-9c3b-424c-d42d-88803ea3dad9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 20) (25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten,Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000,2,input_length=maxlen)) # After embedding layer, the activations have shape(samples,max_len,8)\n",
        "model.add(Flatten()) # Flattens 3D tensors in to 2D tensor of shape (sample, maxlen*8)\n",
        "\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li6GVnuhtmXk",
        "outputId": "af92f596-bdd1-45f0-85d7-e26070d6024a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 20, 2)             20000     \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 40)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 41        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,041\n",
            "Trainable params: 20,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.6801 - acc: 0.6015 - val_loss: 0.6545 - val_acc: 0.6726\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.6034 - acc: 0.7226 - val_loss: 0.5734 - val_acc: 0.7134\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.5239 - acc: 0.7564 - val_loss: 0.5278 - val_acc: 0.7302\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4783 - acc: 0.7775 - val_loss: 0.5081 - val_acc: 0.7414\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4514 - acc: 0.7933 - val_loss: 0.4995 - val_acc: 0.7456\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4336 - acc: 0.8019 - val_loss: 0.4945 - val_acc: 0.7504\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4204 - acc: 0.8069 - val_loss: 0.4920 - val_acc: 0.7570\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4103 - acc: 0.8134 - val_loss: 0.4916 - val_acc: 0.7596\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.4018 - acc: 0.8186 - val_loss: 0.4932 - val_acc: 0.7596\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 0.3951 - acc: 0.8224 - val_loss: 0.4953 - val_acc: 0.7596\n"
          ]
        }
      ]
    }
  ]
}